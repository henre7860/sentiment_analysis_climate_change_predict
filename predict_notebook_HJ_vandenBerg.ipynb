{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9253fac4",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing?\n",
    "\n",
    "Natural language is the language you and I talk in. It could be Hindi, English, Spanish, anything. And we talk about natural language processing, we basically refer to making computers able to process this language, and more importantly understand it and take actions based on it. Now this language can be text based or audio based. Your Google Voice assistant, Siri and even google translator are great examples of this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16218ca",
   "metadata": {},
   "source": [
    "## So what are we doing in the Twitter Sentiment Analysis Project?\n",
    "\n",
    "I am going to build a machine learning model that is able to analyze loads of twitter tweets, and be able to judge the sentiments behind the tweets. We are gonna be analyzing tweets made about climate change, and judging if they are of positive, negative or neutral sentiments. These types of models are massively used by Twitter and Facebook, to filter out to say hate speeches, or other unwanted comments on their platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4ebe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\henre\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "# Database manipulation package\n",
    "import pandas as pd\n",
    "\n",
    "# Natural language Toolkit packages.\n",
    "# Necessary libraries and modules that are \n",
    "# going to help us do the data processing \n",
    "# from the nltk library.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "# Regular expression\n",
    "import re\n",
    "\n",
    "# to make bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Packages to create models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57dc387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "\n",
    "tweets = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d844adfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message</th>\n",
       "      <th>tweetid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PolySciMajor EPA chief doesn't think carbon di...</td>\n",
       "      <td>625221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>It's not like we lack evidence of anthropogeni...</td>\n",
       "      <td>126103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @RawStory: Researchers say we have three ye...</td>\n",
       "      <td>698562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>#TodayinMaker# WIRED : 2016 was a pivotal year...</td>\n",
       "      <td>573736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @SoyNovioDeTodas: It's 2016, and a racist, ...</td>\n",
       "      <td>466954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                            message  tweetid\n",
       "0          1  PolySciMajor EPA chief doesn't think carbon di...   625221\n",
       "1          1  It's not like we lack evidence of anthropogeni...   126103\n",
       "2          2  RT @RawStory: Researchers say we have three ye...   698562\n",
       "3          1  #TodayinMaker# WIRED : 2016 was a pivotal year...   573736\n",
       "4          1  RT @SoyNovioDeTodas: It's 2016, and a racist, ...   466954"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dd3ceaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  0, -1], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de675ad7",
   "metadata": {},
   "source": [
    "Sentiment Description:\n",
    "* 2 News: the tweet links to factual news about climate change\n",
    "\n",
    "* 1 Pro: the tweet supports the belief of man-made climate change\n",
    "\n",
    "* 0 Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "\n",
    "* -1 Anti: the tweet does not believe in man-made climate change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eac52b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15819, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ff6dd",
   "metadata": {},
   "source": [
    " The shape of the data, when you print it, will be shown as (15819, 3). Basically means that there are 15819 tweets in the dataset, with 3 parameters associated with each tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d706d9f2",
   "metadata": {},
   "source": [
    "## Data Pre-Processing is KEY\n",
    "\n",
    "\n",
    "In NLP projects, the most important part is preprocessing the data, so that it is ready for a model to use. That’s what we are doing now.\n",
    "Since there is a unnecessary parameters in the dataset, lets extract and store the input and the output, which is the 'message' and 'sentiment'  column using the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e6b98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['message']\n",
    "y = tweets['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3fd4c4",
   "metadata": {},
   "source": [
    "## Why and What at all do we need to do in Data Processing?\n",
    "\n",
    "Remember your input is just a bunch of words for now. Machine learning models only understand and work on numbers. So we have to convert all the text into numbers.\n",
    "\n",
    "To remove unnecessary words, I am going to use the following techniques:\n",
    "\n",
    "1. Removing Stop Words: Basically words like this, an, a, the, etc that do not affect the meaning of the tweet\n",
    "2. Removing Punctuation: (‘,.*!’) and other punctuation marks that are not really needed by the model\n",
    "3. Stemming: Basically reducing words like ‘jumping, jumped, jump’ into its root word(also called stem), which is jump in this case. Since all variations of the root word convey the same meaning, we don’t need each of the word to be converted into different numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69d8df0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simply store the english stop words,the stemmer function \n",
    "# in different variables in the following lines.\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punct = string.punctuation\n",
    "stemmer = PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08989d65",
   "metadata": {},
   "source": [
    "Now, for the actual data processing, we use the following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad8dde67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data=[]\n",
    "\n",
    "for i in range(len(X)):\n",
    "    tweet=re.sub('[^a-zA-Z]',' ',X.iloc[i])\n",
    "    tweet=tweet.lower().split()\n",
    "    tweet=[stemmer.stem(word) for word in tweet if (word not in stop_words) and (word not in punct)]\n",
    "    tweet=' '.join(tweet)\n",
    "    cleaned_data.append(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190190e",
   "metadata": {},
   "source": [
    "Firstly, we create an empty list called cleaned_data, where will be storing our text data after getting rid of all unnecessary words.\n",
    "\n",
    "We import a library called re(regular expressions), that is gonna help us remove a lot of unnecessary stuff.\n",
    "\n",
    "To begin with, we run a for loop to iterate through each and every tweet in the dataset, at a time. Using re.sub we can substitute and replace things in a sentence. So we are basically taking one tweet at a time, and inside it whatever is not a letter(belonging to a-z or A-Z), will be replaced by an empty space. It will automatically filter out punctuation marks and other non-letters.\n",
    "\n",
    "In the next line, we convert all words into lower cases and split them into a list.\n",
    "\n",
    "Next, we iterate through each word in a tweet, and if that word is not a stop word, we stem that using stemmer.stem(word).\n",
    "\n",
    "After that we join all the words using ‘’.join(tweet), to get a single sentence instead of separated words. And then we simply append that into the cleaned data list that we had created. If you print the cleaned list it should look something like this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fda541ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['polyscimajor epa chief think carbon dioxid main caus global warm wait http co yelvcefxkc via mashabl',\n",
       " 'like lack evid anthropogen global warm',\n",
       " 'rt rawstori research say three year act climat chang late http co wdt kdur f http co z anpt',\n",
       " 'todayinmak wire pivot year war climat chang http co wotxtlcd',\n",
       " 'rt soynoviodetoda racist sexist climat chang deni bigot lead poll electionnight',\n",
       " 'worth read whether believ climat chang http co gglzvnyjun http co afe mah j',\n",
       " 'rt thenat mike penc believ global warm smoke caus lung cancer http co gvwyaauu r',\n",
       " 'rt makeandmendlif six big thing today fight climat chang climat activist http co tymlu dbnm h',\n",
       " 'aceofspadeshq yo nephew inconsol want die old age like perish fieri hellscap climat chang',\n",
       " 'rt paigetweedi offens like believ global warm',\n",
       " 'rt stephenschlegel think go die husband believ climat chang http co sjofon',\n",
       " 'hope peopl vocal climat chang also power home renew energi like goodenergi',\n",
       " 'rt tveitdal percent chanc avoid danger global warm studi find http co xubtqnxhkk http co',\n",
       " 'rt alifaith oh god trump govern remov climat chang page epa websit hour ahead climatemarch http co z',\n",
       " 'fossil fuel giant exxonmobil misl public climat chang harvard academ conclud http co ofc wsu ex',\n",
       " 'rt glblctzn wanna live forev noth climat chang taylorswift zaynmalik http co tui zk gv',\n",
       " 'rt jackholm issu scrub http co vzso n g today civil right climat chang lgbt healthcar',\n",
       " 'rt patagonia elect leader fail approach environ amp climat chang seriou issu worthi urgenc amp action wi',\n",
       " 'rt sensand presid elect believ climat chang million peopl go say mr',\n",
       " 'calum tweet abt reunitingish w cast see repli beg come countri calum goe back rting climat chang',\n",
       " 'rt c citi c mayor repres citizen urg g leader saveourplanet climat chang',\n",
       " 'rt world wildlif climat chang impact wildlif http co oztap cdlk cop earthtomarrakech http co xm xgpvhqi',\n",
       " 'also met guy let us truth climat chang gay peopl exist http co q yomcmzaj',\n",
       " 'http co kynibjmdk scientist say climat chang wipe entir underwat ecosystem climat http co vzvjd oxmi',\n",
       " 'rt tammygrubb obama rais climat chang better vote candid believ scienc obamaunc',\n",
       " 'hate say mental health pretti low menu climat chang hit food product http co kgfpw hmpw',\n",
       " 'bangladesh confront climat chang head http co mtqenbqdut http co itgkuxgefg',\n",
       " 'hey michael vet approv market base strategi tackl climat chang support major cdn cdnpoli',\n",
       " 'salli kohn latest evid climat chang prove smart person http co mhkzogl vt via twitchyteam need say',\n",
       " 'rt startalkradio first public understand climat chang better congress johnholdren corybook neiltyson explain htt',\n",
       " 'jnp ftw scientist denounc climat chang choic',\n",
       " 'rt honey guid global warm pari pact us role demforc theresist uniteblu http co ryiblwajz',\n",
       " 'rt latim atmospher river fuel climat chang could decim wild oyster san francisco bay http co p lzbhlu k http',\n",
       " 'deni climat chang ignor basic scienc',\n",
       " 'bgr china practic say trump lie climat chang http co rz htc',\n",
       " 'rt berniesand imvotingbecaus futur planet stake hillari clinton combat climat chang donald trump think',\n",
       " 'rt independ trump team remov climat chang data white hous websit may break law',\n",
       " 'rt deborahehart toxic soil aquif vast amt wast wast water runaway climat chang part ga frack',\n",
       " 'rt bettybow america climat chang unproven peopl sure guy call noah fit world anim hi',\n",
       " 'dealt simpl issu like climat chang energi polici complex issu mal vs tone qanda',\n",
       " 'rt foramerica protest support climat chang womansmarch part help environ http',\n",
       " 'climat target nation play long game fight global warm http co dnegw vfjd via conversationedu',\n",
       " 'rt thedailybeast trump climat chang denial could cost us trillion http co klttt sxrg http co go ycr',\n",
       " 'rt andrewsharp win probabl bullshit man saw nba final knew global warm real',\n",
       " 'rt washingtonpost alaskan tundra fill atmospher carbon dioxid worsen climat chang http co nsoimuweax',\n",
       " 'rt thetorontosun sunlorri indian environmentalist call dicaprio documentari climat chang',\n",
       " 'realdonaldtrump carbon tax globalist idea enslav world popul whole reason propag global warm',\n",
       " 'anyon support trump view climat chang non pollut croni pleas step forward resist impeach http co ofob utail',\n",
       " 'rt stevesgoddard wind close mph area afternoon would blame climat chang except happen pr',\n",
       " 'rt starbuck world food suppli risk climat chang threaten intern trade warn expert http co rwcw xf lr http',\n",
       " 'rt foxnew macron charm may chang trump mind climat chang http co hxdjqwgbe http co idj nzq',\n",
       " 'rt stephenschlegel think go die husband believ climat chang http co sjofon',\n",
       " 'chri initi talk climat chang think three second europea http co vwlqici h',\n",
       " 'approv execut order realdonaldtrump due sign climat chang environ http co nr nqbjl',\n",
       " 'trump believ global warm go presid unit state america fuck',\n",
       " 'rt kelkulu ironi florida state danger wash away due rise sea level elect guy deni climat chang',\n",
       " 'misslizzynj lmao snowflak complain snowflak winter global warm',\n",
       " 'rt dawn dawn one arnold schwarzenegg vehicl whine climat chang maid illeg http',\n",
       " 'glennf call great space cylind save global warm',\n",
       " 'rt greenpeaceeasia pollut india china reach stratospher could speed global warm',\n",
       " 'rt unep citiesclimfin lay citi subnat bodi financ solut climat chang',\n",
       " 'hous republican say climat chang real time fight ninaburleigh http co vnayfuhglw',\n",
       " 'global warm hoax though http co pzugb wfwt',\n",
       " 'rt kylegriffin nyt review draft climat chang rpt fed scientist directli contradict admin global warm claim http',\n",
       " 'rt fuckofflain jjxrri spacex quebanjesu antarctica big lmao global warm real like nigga use air condition',\n",
       " 'two cent global polit leadership climat chang trump era http co iqxyti',\n",
       " 'rt bbcbreak uk govern sign pari agreement world first comprehens treati tackl climat chang http co hdafst pfc',\n",
       " 'rt hope china tell trump climat chang hoax invent http co tx pzxyzd via busi lol china school',\n",
       " 'rt motherjon race watch care global warm http co hikysg r http co syxkddd xe',\n",
       " 'uscgpacificnw nsf icebreak ship primari caus climat chang let arctic freez amp stay away',\n",
       " 'rt kelkulu ironi florida state danger wash away due rise sea level elect guy deni climat chang',\n",
       " 'global warm exist club penguin shut http co os c mfyx',\n",
       " 'rt profpcdoherti need economist incorpor realiti climat chang think amp need listen http co liv',\n",
       " 'rt ushrn fight human right also face disastr impact climat chang http co q ag ex oq',\n",
       " 'rt cernovich climat chang real http co fecivoogma',\n",
       " 'rt ahamiltonspirit question climat chang man made doubt seen inconveni truth pleas',\n",
       " 'alphageekst r process climat chang occur natur extrem level eventu sever drought',\n",
       " 'rt roschneid neoliber con us fight climat chang individu martin lukac http co g mhojzxa',\n",
       " 'rt davidsirota must read rick perri order texa legislatur kill bill requir state agenc prepar climat chang http',\n",
       " 'rt sensand presid elect believ climat chang frighten countri world',\n",
       " 'hot due global warm',\n",
       " 'rt ldn environ short term opp work climat chang amp energi send us cv q environ london gov uk h',\n",
       " 'rt markbaileymp fed energi envt minist joshfrydenberg refus allow term climat chang coag energi council communiqu despit',\n",
       " 'rt jadorelacoutur fact leonardo dicaprio met trump discuss climat chang man',\n",
       " 'rt verg googl earth timelaps updat illustr year climat chang http co ktduwzasfc http co nzz tyt',\n",
       " 'rt sethmacfarlan hrc propos instal half billion solar panel end first term trump think climat chang hoax',\n",
       " 'beforetheflood everyon must watch peopl must understand global warm real leodicaprio http co vtqnt f ni',\n",
       " 'shake climat chang noth ever http co yhpje lyp',\n",
       " 'rt smithsonianenv serc roy rich amp colleagu creat buzz germani global warm simul http co shqnjexh',\n",
       " 'rt earthjustic noaa report nd hottest year histori one reason fight climat chang denier trump admin gt',\n",
       " 'conserv love deni scienc huh climat chang believ fertil egg automat human life',\n",
       " 'senior air qualiti climat chang specialist senior air http co tsqzmjwlh airqual climatechang greenhousega',\n",
       " 'come check event climat chang srhpl http co yviyn lo',\n",
       " 'think million could gone toward allevi poverti reduc climat chang improv healthcar call mad',\n",
       " 'cnn protest global warm stupid',\n",
       " 'rt ronaldklain trump decid pari briancdees amp team washingtonpost warn climat chang epidem http',\n",
       " 'rt savingocean lack climat chang action potenti hurt futur fishermen job http co md qw vtc',\n",
       " 'yet anoth trump advisor clueless climat chang climateprogress http co apamiibdjj',\n",
       " 'rt stephenschlegel think go die husband believ climat chang http co sjofon',\n",
       " 'u environment agenc chief say human contribut global warm http co assz eszx']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaef51f",
   "metadata": {},
   "source": [
    "In this, climat and chang is the subject for which the tweets are made. We’ll also remove them later, as they do not play a role in the sentiment of the tweet. Also you’ll notice weird words like ‘tsqzmjwlh’ which do not make sense. That is because people in tweets will often use incorrect spellings of words and also the stemmer function sometimes produces words that are not real English words, but that is something that we will just roll with for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3bc1b9",
   "metadata": {},
   "source": [
    "## Bringing in the Bag of Words Approach\n",
    "\n",
    "Now that input is clean and ready, we convert it into numbers using something called as the ‘Bag of Words’ approach. Basically we create a matrix table, where each row represents a sentence and each word will have separate column for itself that represents it’s frequency.\n",
    "\n",
    "One con about this method that you might not notice is that the order of the sentence is lost. There are other approaches to counter this, but we are just going to stick with this method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b17568c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vector = CountVectorizer(max_features = 3000, stop_words = \\\n",
    "                               ['chang','climat', 'http','co'])\n",
    "X_fin = count_vector.fit_transform(cleaned_data).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f8df5",
   "metadata": {},
   "source": [
    "The Count Vectorizer function converts a list of words into bag of words, however notice that we specify something called as the max features to it. Basically as you might have seen in the bag of words illustration table, each word will have separate column. This number of columns can explode into large numbers in big datasets.\n",
    "\n",
    "To avoid this we set the max columns as 3000, and basically keep the maximum occurring 3000 words. Also we set the stop_words parameter to the name of the the subject(climate change) and url words, as we want to remove that from the tweets as well.\n",
    "\n",
    "Finall the cv.fit_transform function takes the cleaned_data and converts it into the bag of words that we wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a92df",
   "metadata": {},
   "source": [
    "Let’s have a look at the output column ‘y’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afa30cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    1\n",
       "4    1\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32650293",
   "metadata": {},
   "source": [
    "As we can see the output is already in numeric format, so no processing is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d4cce5",
   "metadata": {},
   "source": [
    "Let’s have a look at the output column ‘y’"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c9ead5",
   "metadata": {},
   "source": [
    "## Build the NLP models\n",
    "\n",
    "I am going to use the Multinomial Naive Bayes model to figure out the relationship between the input and the output. Multinomial NB is a supervised learning algorithm that works really well for text based data. \n",
    "\n",
    "Now to build the model we split the dataset into a training and testing section(testing size=30% of the actual data). To actually fit the model, we call the model.fit function and supply it the training input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3b5f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_fin,y,test_size=0.3)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#c_report = classification_report(y_test, y_pred)\n",
    "\n",
    "f1score = round(f1_score(y_test, y_pred, average= 'macro'),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4940a0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score: 0.61959\n"
     ]
    }
   ],
   "source": [
    "#print(c_report)\n",
    "print('macro f1 score:', f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e346d0",
   "metadata": {},
   "source": [
    "The accuracy is not great as as macro f1 score of greater than 0.7 is requiered. To improve it, there are several things that we could do, for example using something called as lemmatization instead of stemming, using a balanced dataset, and trying with different machine learning models as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f29b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384d4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83233960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49779eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9edda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad564e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56348524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73d926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28f2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
